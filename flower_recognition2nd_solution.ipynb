{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization\n",
    "from tensorflow.python.keras.layers.convolutional import Conv2D, MaxPooling2D \n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.applications.resnet import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd() + '/flowers/flowers/'\n",
    "\n",
    "categories = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = []\n",
    "for category in categories:\n",
    "    flower_folder = os.path.join(base_path, category)\n",
    "    #print (flower_folder)\n",
    "    file_names = os.listdir(flower_folder)\n",
    "    full_path = [os.path.join(flower_folder, file_name) for file_name in file_names]\n",
    "    fnames.append(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('length for each category:', [len(f) for f in fnames])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "for names in fnames:\n",
    "    one_category_images = [cv2.imread(name) for name in names if (cv2.imread(name)) is not None]\n",
    "    images.append(one_category_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of images for each category:', [len(f) for f in images])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the minimal shape for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,imgs in enumerate(images):\n",
    "    shapes = [img.shape for img in imgs]\n",
    "    widths = [shape[0] for shape in shapes]\n",
    "    heights = [shape[1] for shape in shapes]\n",
    "    print('%d,%d is the min shape for %s' % (np.min(widths), np.min(heights), categories[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert cv2 BGR format to RGB for showing\n",
    "def cvtRGB(img):\n",
    "    return cv2.cvtColor(img.copy(), cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample images\n",
    "plt.figure(figsize=(15,10))\n",
    "for i, imgs in enumerate(images):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    idx = np.random.randint(len(imgs))\n",
    "    plt.imshow(cvtRGB(imgs[idx]))\n",
    "    plt.grid('off')\n",
    "    plt.grid(False)\n",
    "    plt.title(categories[i]+' '+str(idx))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize all the images to 256x256\n",
    "img_width, img_height = 256, 256\n",
    "\n",
    "img = images[3][659]\n",
    "print(img.shape)\n",
    "resized_img = resize(img, (img_width, img_height, 3))\n",
    "resized_img2 = cv2.resize(img,(img_width, img_height), interpolation = cv2.INTER_CUBIC)\n",
    "print(resized_img.shape)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(2,2,1)\n",
    "plt.title('original image (BGR-channel)')\n",
    "plt.grid(False)\n",
    "plt.imshow(img)\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('original image (RGB-channel)')\n",
    "plt.grid(False)\n",
    "plt.imshow(cvtRGB(img))\n",
    "plt.subplot(2,2,3)\n",
    "plt.title('resized by skimage (BGR-channel)')\n",
    "plt.grid(False)\n",
    "plt.imshow((resized_img))\n",
    "plt.subplot(2,2,4)\n",
    "plt.title('resized by opencv (RGB-channel)')\n",
    "plt.grid(False)\n",
    "plt.imshow(cvtRGB(resized_img2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply resize to all images\n",
    "resized_images = []\n",
    "for i,imgs in enumerate(images):\n",
    "    resized_images.append([cv2.resize(img, (img_width, img_height), interpolation = cv2.INTER_CUBIC) for img in imgs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset to 80% of training and 20% of validation\n",
    "train_images = []\n",
    "val_images = []\n",
    "for imgs in resized_images:\n",
    "    train, test = train_test_split(imgs, train_size=0.8, test_size=0.2)\n",
    "    train_images.append(train)\n",
    "    val_images.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creat labels\n",
    "len_train_images = [len(imgs) for imgs in train_images]\n",
    "print(len_train_images)\n",
    "print('sum of train images:', np.sum(len_train_images))\n",
    "train_categories = np.zeros((np.sum(len_train_images)), dtype='uint8')\n",
    "for i in range(5):\n",
    "    if i is 0:\n",
    "        train_categories[:len_train_images[i]] = i\n",
    "    else:\n",
    "        train_categories[np.sum(len_train_images[:i]):np.sum(len_train_images[:i+1])] = i\n",
    "        \n",
    "len_val_images = [len(imgs) for imgs in val_images]\n",
    "print(len_val_images)\n",
    "print('sum of val_images:', np.sum(len_val_images))\n",
    "val_categories = np.zeros((np.sum(len_val_images)), dtype='uint8')\n",
    "for i in range(5):\n",
    "    if i is 0:\n",
    "        val_categories[:len_val_images[i]] = i\n",
    "    else:\n",
    "        val_categories[np.sum(len_val_images[:i]):np.sum(len_val_images[:i+1])] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert image data to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_train_imgs = []\n",
    "tmp_val_imgs = []\n",
    "for imgs in train_images:\n",
    "    tmp_train_imgs += imgs\n",
    "for imgs in val_images:\n",
    "    tmp_val_imgs += imgs\n",
    "train_images = np.array(tmp_train_imgs)\n",
    "val_images = np.array(tmp_val_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before converting')\n",
    "print('train data:', train_images.shape)\n",
    "print('train labels:', train_categories.shape)\n",
    "\n",
    "train_data = train_images.astype('float32')\n",
    "val_data = val_images.astype('float32')\n",
    "train_labels = np_utils.to_categorical(train_categories, len(categories))\n",
    "val_labels = np_utils.to_categorical(val_categories, len(categories))\n",
    "print()\n",
    "print('After converting')\n",
    "print('train data:', train_data.shape)\n",
    "print('train labels:', train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the dataset (set a seed and randomize both the data and the labels)\n",
    "seed = 100\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(train_data)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(train_labels)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(val_data)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[:3400]\n",
    "train_labels = train_labels[:3400]\n",
    "val_data = val_data[:860]\n",
    "val_labels = val_labels[:860]\n",
    "print('shape of train data:', train_data.shape)\n",
    "print('shape of train labels:', train_labels.shape)\n",
    "print('shape of val data:', val_data.shape)\n",
    "print('shape of val labels:', val_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dl libraraies\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# specifically for cnn\n",
    "from keras.layers import Dropout, Flatten,Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # modelling starts using a CNN.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (256,256,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    " \n",
    "\n",
    "model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same',activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(5, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding rescale, rotation_range, width_shift_range, height_shift_range,\n",
    "# shear_range, zoom_range, and horizontal flip to our ImageDataGenerator\n",
    "batch_size=128\n",
    "epochs=10\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.4,\n",
    "    height_shift_range=0.4,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "#     rotation_range=40,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     shear_range=0.2,\n",
    "#     zoom_range=0.2,\n",
    "#     horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Flow training images in batches of 32 using train_datagen generator\n",
    "train_generator = train_datagen.flow(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow(\n",
    "    val_data,\n",
    "    val_labels,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "model_scratch_info = model.fit_generator(\n",
    "    generator=train_generator, \n",
    "    steps_per_epoch=len(train_data)/batch_size,   # -> 106 # images 3392 = steps * batch_size = 106 * 32 \n",
    "    epochs=epochs, \n",
    "    validation_steps=len(val_data)/batch_size, # -> 26 # images 832 = steps * batch_size = 26 * 32\n",
    "    validation_data=val_generator, \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "duration = end - start\n",
    "print ('\\n model_scratch took %0.2f seconds (%0.1f minutes) to train for %d epochs'%(duration, duration/60, epochs1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
